{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57b8ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/workspace/tez\")\n",
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import copy\n",
    "import tez\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AdamW, AutoConfig, AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac09e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path_list(base_dir):\n",
    "    \"\"\"\n",
    "    从文件夹中获取 model.pt 的路径\n",
    "    \"\"\"\n",
    "    model_lists = [file for file in Path(base_dir).iterdir() if file.stem.startswith(\"model\")]\n",
    "\n",
    "    return model_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f508e457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackModel(tez.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name,\n",
    "        num_labels,\n",
    "        dynamic_merge_layers,\n",
    "        decoder=\"softmax\",\n",
    "        max_len=4096,\n",
    "        span_num_labels=8\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.dynamic_merge_layers = dynamic_merge_layers\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.decoder = decoder\n",
    "\n",
    "        hidden_dropout_prob: float = 0.1\n",
    "        layer_norm_eps: float = 1e-7\n",
    "\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "\n",
    "        config.update(\n",
    "            {\n",
    "                \"output_hidden_states\": True,\n",
    "                \"hidden_dropout_prob\": hidden_dropout_prob,\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "                \"add_pooling_layer\": False,\n",
    "                \"num_labels\": self.num_labels,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(model_name, config=config)\n",
    "        if self.dynamic_merge_layers:\n",
    "            self.layer_logits = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        if self.decoder == \"span\":\n",
    "            self.start_fc = nn.Linear(config.hidden_size, span_num_labels)\n",
    "            self.end_fc = nn.Linear(config.hidden_size, span_num_labels)\n",
    "        else:\n",
    "            self.output = nn.Linear(config.hidden_size, self.num_labels)\n",
    "            if self.decoder == \"crf\":\n",
    "                self.crf = CRF(num_tags=num_labels, batch_first=True)\n",
    "        \n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids=None, targets=None):\n",
    "        if token_type_ids:\n",
    "            transformer_out = self.transformer(ids, mask, token_type_ids, output_hidden_states=self.dynamic_merge_layers)\n",
    "        else:\n",
    "            transformer_out = self.transformer(ids, mask, output_hidden_states=self.dynamic_merge_layers)\n",
    "\n",
    "        if self.decoder == \"crf\" and transformer_out.last_hidden_state.shape[1] != ids.shape[1]:\n",
    "            mask_add = torch.zeros((mask.shape[0],  transformer_out.hidden_states[-1].shape[1] - ids.shape[1])).to(mask.device)\n",
    "            mask = torch.cat((mask, mask_add), dim=-1)\n",
    "        if self.dynamic_merge_layers:\n",
    "            layers_output = torch.cat([torch.unsqueeze(layer, 2) for layer in transformer_out.hidden_states[self.merge_layers_num:]], dim=2)\n",
    "            layers_logits = self.layer_logits(layers_output)\n",
    "            layers_weights = torch.transpose(torch.softmax(layers_logits, dim=-1), 2, 3)\n",
    "            sequence_output = torch.squeeze(torch.matmul(layers_weights, layers_output), 2)\n",
    "        else:\n",
    "            sequence_output = transformer_out.last_hidden_state\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "\n",
    "        if self.decoder == \"span\":\n",
    "            start_logits = self.start_fc(sequence_output)\n",
    "            end_logits = self.end_fc(sequence_output)\n",
    "            logits = (start_logits, end_logits)\n",
    "            probs = None\n",
    "        else:\n",
    "            logits = self.output(sequence_output)\n",
    "            if self.decoder == \"softmax\":\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "            elif self.decoder == \"crf\":\n",
    "                probs = self.crf.decode(emissions=logits, mask=mask.byte())\n",
    "            else:\n",
    "                raise ValueException(\"except decoder in [softmax, crf]\")\n",
    "        loss = 0\n",
    "\n",
    "        return {\n",
    "            \"preds\": probs,\n",
    "            \"logits\": logits,\n",
    "            \"loss\": loss,\n",
    "            \"metric\": {}\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7afd057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetModel: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = FeedbackModel(\n",
    "    model_name=\"xlnet-base-cased\",\n",
    "    dynamic_merge_layers=False,\n",
    "    num_labels=15,\n",
    "    decoder=\"crf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52faa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swa(model, model_dir):\n",
    "    \"\"\"\n",
    "    swa 滑动平均模型，一般在训练平稳阶段再使用 SWA\n",
    "    \"\"\"\n",
    "    model_path_list = get_model_path_list(model_dir)\n",
    "\n",
    "    swa_model = copy.deepcopy(model)\n",
    "    swa_n = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ckpt in model_path_list:\n",
    "            print(f'Load model from {_ckpt}')\n",
    "            model.load_state_dict(torch.load(_ckpt, map_location=torch.device('cpu')))\n",
    "            tmp_para_dict = dict(model.named_parameters())\n",
    "\n",
    "            alpha = 1. / (swa_n + 1.)\n",
    "\n",
    "            for name, para in swa_model.named_parameters():\n",
    "                para.copy_(tmp_para_dict[name].data.clone() * alpha + para.data.clone() * (1. - alpha))\n",
    "\n",
    "            swa_n += 1\n",
    "\n",
    "    # use 100000 to represent swa to avoid clash\n",
    "    swa_model_dir = os.path.join(model_dir, f'checkpoint-100000')\n",
    "    if not os.path.exists(swa_model_dir):\n",
    "        os.mkdir(swa_model_dir)\n",
    "\n",
    "    print(f'Save swa model in: {swa_model_dir}')\n",
    "\n",
    "    swa_model_path = os.path.join(swa_model_dir, 'model.bin')\n",
    "\n",
    "    torch.save(swa_model.state_dict(), swa_model_path)\n",
    "\n",
    "    return swa_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00559fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model from /workspace/data-0225-xlnet-base-swa/model_2.bin_epoch1\n",
      "Load model from /workspace/data-0225-xlnet-base-swa/model_2.bin_epoch2\n",
      "Load model from /workspace/data-0225-xlnet-base-swa/model_2.bin_epoch5\n",
      "Save swa model in: /workspace/data-0225-xlnet-base-swa/checkpoint-100000\n"
     ]
    }
   ],
   "source": [
    "swa_model = swa(model, \"/workspace/data-0225-xlnet-base-swa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36c2ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
